# ğŸ”— Guia Completo: Compartilhar APIs via MCP Server

## ğŸ¯ Objetivo
Permitir que seu amigo use suas API keys atravÃ©s de um servidor MCP compartilhado, sem expor as chaves no GitHub.

## ğŸ—ï¸ Arquitetura

```
[VocÃª] â†’ MCP Server â†’ Suas APIs (OpenAI, Gemini, OpenRouter)
   â†‘
[Seu Amigo] â†’ Cliente MCP â†’ Conecta ao seu servidor
```

## ğŸ“‹ Passo a Passo

### ğŸ”§ **VOCÃŠ (Hospedeiro das APIs):**

#### 1. **Preparar o Servidor Compartilhado**
```bash
cd /home/lucas-junges/Documents/evolux-engine/mcp-llm-server
```

#### 2. **Configurar Token de Acesso**
```bash
# Adicionar token ao .env
echo "SHARED_ACCESS_TOKEN=seu-token-super-secreto-123" >> .env
```

#### 3. **Descobrir seu IP Local**
```bash
# Ver seu IP na rede local
ip route get 1.1.1.1 | awk '{print $7; exit}'

# OU
hostname -I | awk '{print $1}'

# Exemplo de resultado: 192.168.1.100
```

#### 4. **Executar o Servidor Compartilhado**
```bash
source venv/bin/activate
python3 shared_mcp_server.py
```

VocÃª verÃ¡ algo como:
```
ğŸš€ Iniciando Shared MCP LLM Server...
ğŸ”‘ Token de acesso: seu-token-super-secreto-123
ğŸ“¡ Servidor disponÃ­vel para conexÃµes externas
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 * Running on all addresses (0.0.0.0)
 * Running on http://192.168.1.100:8080
```

#### 5. **Compartilhar InformaÃ§Ãµes com seu Amigo**
Envie para ele:
- **IP:** `192.168.1.100` (ou o que aparecer)
- **Porta:** `8080`
- **Token:** `seu-token-super-secreto-123`

---

### ğŸ‘¥ **SEU AMIGO (Cliente):**

#### 1. **Baixar os Arquivos**
```bash
# Se vocÃª subiu no GitHub (sem .env)
git clone https://github.com/seu-usuario/mcp-llm-server.git
cd mcp-llm-server

# OU baixar apenas os arquivos necessÃ¡rios:
# - claude_mcp_client.py
# - client_mcp.py
```

#### 2. **Instalar DependÃªncias**
```bash
pip install requests mcp
```

#### 3. **Configurar Cliente**
Editar `claude_mcp_client.py`:
```python
# Linha 17-18, substituir:
self.server_url = "http://192.168.1.100:8080"  # IP que vocÃª forneceu
self.access_token = "seu-token-super-secreto-123"  # Token que vocÃª forneceu
```

#### 4. **Testar ConexÃ£o**
```bash
python3 client_mcp.py
```

#### 5. **Registrar no Claude Code**
```bash
claude mcp add shared-llm 'python3 /caminho/para/claude_mcp_client.py'
```

#### 6. **Usar no Claude Code**
```bash
@shared-llm shared_llm_chat provider=openai message="OlÃ¡, como vocÃª estÃ¡?"
@shared-llm shared_llm_chat provider=gemini message="Explique inteligÃªncia artificial"
@shared-llm shared_list_models
```

---

## ğŸ›¡ï¸ **OpÃ§Ã£o 2: Usando ngrok (Internet)**

Se vocÃªs nÃ£o estÃ£o na mesma rede, use ngrok para expor o servidor:

### **VOCÃŠ:**
```bash
# Instalar ngrok
sudo snap install ngrok

# Expor servidor
ngrok http 8080
```

Isso darÃ¡ uma URL pÃºblica como: `https://abc123.ngrok.io`

### **SEU AMIGO:**
```python
# Usar a URL do ngrok
self.server_url = "https://abc123.ngrok.io"
```

---

## ğŸ” **OpÃ§Ã£o 3: Servidor na Nuvem**

### **Deploy no Heroku (VOCÃŠ):**

1. **Criar app Heroku:**
```bash
heroku create mcp-llm-shared
```

2. **Configurar variÃ¡veis:**
```bash
heroku config:set OPENAI_API_KEY=sua-chave
heroku config:set OPENROUTER_API_KEY=sua-chave  
heroku config:set GEMINI_API_KEY=sua-chave
heroku config:set SHARED_ACCESS_TOKEN=token-super-secreto
```

3. **Deploy:**
```bash
git push heroku main
```

4. **Compartilhar URL:**
```
https://mcp-llm-shared.herokuapp.com
```

---

## ğŸ§ª **Testando o Sistema**

### **Teste Local (SEU AMIGO):**
```bash
python3 client_mcp.py
```

SaÃ­da esperada:
```
ğŸ”„ Testando conexÃ£o com servidor compartilhado...
ğŸ“Š Status do servidor: {'status': 'healthy', 'service': 'Shared LLM MCP Server'}
ğŸ“‹ Modelos disponÃ­veis: {...}

ğŸ§ª Testando openai...
âœ… openai: OlÃ¡! Como posso ajudar vocÃª hoje?

ğŸ§ª Testando gemini...
âœ… gemini: Como um modelo de linguagem...

ğŸ§ª Testando openrouter...
âœ… openrouter: Aqui vai uma piada...
```

### **Teste no Claude Code:**
```bash
@shared-llm shared_llm_chat provider=openai message="Conte sobre machine learning"
```

---

## ğŸ”’ **SeguranÃ§a**

### **Medidas Implementadas:**
- âœ… **Token de Acesso:** Apenas quem tem o token pode usar
- âœ… **Rate Limiting:** Evita abuso (pode ser adicionado)
- âœ… **CORS:** Controla origens permitidas
- âœ… **Timeout:** Evita requests longos

### **Melhorias Opcionais:**
```python
# Adicionar ao servidor para mais seguranÃ§a:
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address

limiter = Limiter(
    app,
    key_func=get_remote_address,
    default_limits=["100 per hour"]
)
```

---

## ğŸ“Š **Vantagens desta SoluÃ§Ã£o**

### âœ… **Para VocÃª:**
- API keys nunca saem do seu ambiente
- Controle total sobre o uso
- Pode revogar acesso a qualquer momento
- Monitora uso das APIs

### âœ… **Para seu Amigo:**
- NÃ£o precisa ter prÃ³prias API keys
- Usa atravÃ©s do Claude Code normalmente
- Interface familiar
- Sem configuraÃ§Ã£o complexa

### âœ… **Para o Projeto:**
- GitHub limpo (sem API keys)
- CÃ³digo compartilhÃ¡vel
- ColaboraÃ§Ã£o segura
- Facilmente escalÃ¡vel

---

## ğŸ“ **Troubleshooting**

### **Problema: ConexÃ£o recusada**
```bash
# Verificar se servidor estÃ¡ rodando
curl http://seu-ip:8080/health

# Verificar firewall
sudo ufw allow 8080
```

### **Problema: Token invÃ¡lido**
```bash
# Verificar se token estÃ¡ correto nos dois lados
echo $SHARED_ACCESS_TOKEN
```

### **Problema: Timeout**
```bash
# Verificar se redes estÃ£o conectadas
ping seu-ip
```

---

## ğŸ‰ **Resultado Final**

Com esta configuraÃ§Ã£o:
- âœ… **API keys protegidas** (nunca vÃ£o para GitHub)
- âœ… **Compartilhamento seguro** com token de acesso
- âœ… **Uso normal no Claude Code** para seu amigo
- âœ… **Controle total** das suas APIs
- âœ… **ColaboraÃ§Ã£o facilitada** no projeto

**ğŸš€ Agora vocÃªs podem colaborar sem expor API keys!**