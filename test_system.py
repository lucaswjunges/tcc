#!/usr/bin/env python3
"""
Script de teste para verificar se o sistema Evolux Engine est√° funcionando corretamente
"""

import os
import sys
import asyncio
from pathlib import Path

def test_basic_imports():
    """Testa importa√ß√µes b√°sicas"""
    print("üîß Testando importa√ß√µes b√°sicas...")
    
    try:
        import evolux_engine
        print("‚úÖ evolux_engine importado com sucesso")
        
        from evolux_engine import (
            AdvancedSystemConfig,
            ConfigurationManager,
            EnterpriseObservabilityService,
            AdvancedContextManager,
            SecurityGateway,
            ModelRouter,
            LLMFactory,
            PromptEngine,
            IntelligentProjectScaffolding
        )
        print("‚úÖ Todas as classes principais importadas com sucesso")
        
        return True
    except Exception as e:
        print(f"‚ùå Erro na importa√ß√£o: {e}")
        return False

def test_configuration():
    """Testa sistema de configura√ß√£o"""
    print("\n‚öôÔ∏è  Testando sistema de configura√ß√£o...")
    
    try:
        from evolux_engine import AdvancedSystemConfig, ConfigurationManager
        
        # Teste com API key mock diretamente
        os.environ['EVOLUX_OPENROUTER_API_KEY'] = 'test_key_1234567890'
        config = AdvancedSystemConfig()
        print(f"‚úÖ Configura√ß√£o com API key funcionando")
        
        # Verificar se API key est√° sendo lida
        api_key = config.get_api_key('openrouter')
        if api_key:
            print(f"‚úÖ API key OpenRouter detectada: {api_key[:8]}***")
        else:
            print("‚ùå API key OpenRouter n√£o detectada")
        print(f"‚úÖ Configura√ß√£o criada com sucesso")
        print(f"   - Provedor padr√£o: {config.default_llm_provider}")
        print(f"   - Modelo executor: {config.default_model_executor}")
        print(f"   - Timeout: {config.request_timeout}s")
        
        # Teste do ConfigurationManager
        config_manager = ConfigurationManager()
        loaded_config = config_manager.load_configuration()
        print(f"‚úÖ ConfigurationManager funcionando")
        
        return True
    except Exception as e:
        print(f"‚ùå Erro no teste de configura√ß√£o: {e}")
        return False

def test_observability():
    """Testa sistema de observabilidade"""
    print("\nüìä Testando sistema de observabilidade...")
    
    try:
        from evolux_engine import EnterpriseObservabilityService, AdvancedSystemConfig
        
        # Configura√ß√£o mock
        os.environ['EVOLUX_OPENROUTER_API_KEY'] = 'test_key_1234567890'
        config = AdvancedSystemConfig()
        
        # Criar servi√ßo de observabilidade
        obs_service = EnterpriseObservabilityService(config)
        
        # Testar m√©tricas
        from evolux_engine.services.enterprise_observability import MetricType
        obs_service.record_metric("test.counter", 1.0, MetricType.COUNTER)
        obs_service.set_gauge("test.gauge", 42.5)
        print("‚úÖ Registro de m√©tricas funcionando")
        
        # Testar tracing
        span_id = obs_service.start_trace("test_operation")
        obs_service.add_trace_log(span_id, "Test log entry")
        obs_service.finish_trace(span_id, "completed")
        print("‚úÖ Sistema de tracing funcionando")
        
        # Testar m√©tricas de performance
        metrics = obs_service.get_performance_metrics()
        print(f"‚úÖ M√©tricas de performance obtidas: CPU {metrics.cpu_percent}%")
        
        return True
    except Exception as e:
        print(f"‚ùå Erro no teste de observabilidade: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_security():
    """Testa sistema de seguran√ßa"""
    print("\nüîí Testando sistema de seguran√ßa...")
    
    try:
        from evolux_engine.security.security_gateway import SecurityGateway, SecurityLevel
        
        # Criar gateway de seguran√ßa
        security = SecurityGateway(SecurityLevel.STRICT)
        
        # Testar comando seguro
        result = asyncio.run(security.validate_command("ls -la"))
        print(f"‚úÖ Comando seguro validado: {result.is_safe}")
        
        # Testar comando perigoso
        result = asyncio.run(security.validate_command("rm -rf /"))
        print(f"‚úÖ Comando perigoso bloqueado: {not result.is_safe}")
        print(f"   Raz√£o: {result.blocked_reason}")
        
        # Estat√≠sticas
        stats = security.get_security_stats()
        print(f"‚úÖ Estat√≠sticas de seguran√ßa: {stats['total_validations']} valida√ß√µes")
        
        return True
    except Exception as e:
        print(f"‚ùå Erro no teste de seguran√ßa: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_llm_routing():
    """Testa sistema de roteamento de LLM"""
    print("\nü§ñ Testando sistema de roteamento de LLM...")
    
    try:
        from evolux_engine import ModelRouter, LLMFactory
        from evolux_engine.llms.model_router import TaskCategory
        
        # Criar router
        router = ModelRouter()
        
        # Testar sele√ß√£o de modelo
        model = router.select_model(TaskCategory.CODE_GENERATION)
        print(f"‚úÖ Modelo selecionado para gera√ß√£o de c√≥digo: {model}")
        
        # Testar fallback
        fallback = router.get_fallback_model(model, TaskCategory.CODE_GENERATION)
        print(f"‚úÖ Modelo de fallback: {fallback}")
        
        # Estat√≠sticas
        stats = router.get_routing_stats()
        print(f"‚úÖ Estat√≠sticas do router: {stats['total_models']} modelos dispon√≠veis")
        
        return True
    except Exception as e:
        print(f"‚ùå Erro no teste de roteamento LLM: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_project_scaffolding():
    """Testa sistema de scaffolding"""
    print("\nüèóÔ∏è  Testando sistema de scaffolding...")
    
    try:
        from evolux_engine import IntelligentProjectScaffolding, AdvancedSystemConfig
        from evolux_engine.core.project_scaffolding import ProjectType, Framework
        
        # Configura√ß√£o
        os.environ['EVOLUX_OPENROUTER_API_KEY'] = 'test_key_1234567890'
        config = AdvancedSystemConfig()
        
        # Criar sistema de scaffolding
        scaffolding = IntelligentProjectScaffolding(config)
        
        # Analisar goal
        analysis = scaffolding.analyze_project_goal("Create a REST API using Flask")
        print(f"‚úÖ An√°lise de goal conclu√≠da:")
        print(f"   - Tipo detectado: {analysis['project_type']}")
        print(f"   - Linguagem: {analysis['language']}")
        print(f"   - Features: {analysis['detected_features']}")
        
        # Gerar scaffold
        scaffold = scaffolding.generate_project_scaffold(
            goal="Create a simple Flask web application",
            project_name="TestApp",
            output_dir="/tmp/test_scaffold",
            force_type=ProjectType.WEB_APPLICATION,
            force_framework=Framework.FLASK
        )
        print(f"‚úÖ Scaffold gerado: {scaffold.name}")
        print(f"   - Framework: {scaffold.framework}")
        print(f"   - Depend√™ncias: {len(scaffold.dependencies.get('python', []))}")
        
        return True
    except Exception as e:
        print(f"‚ùå Erro no teste de scaffolding: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_context_manager():
    """Testa sistema de gerenciamento de contexto"""
    print("\nüìù Testando sistema de gerenciamento de contexto...")
    
    try:
        from evolux_engine import AdvancedContextManager, AdvancedSystemConfig
        
        # Configura√ß√£o
        os.environ['EVOLUX_OPENROUTER_API_KEY'] = 'test_key_1234567890'
        config = AdvancedSystemConfig()
        
        # Criar context manager
        context_manager = AdvancedContextManager(config)
        
        # Criar contexto de teste
        context = context_manager.create_new_project_context(
            goal="Test project for system validation",
            project_name="Test Project"
        )
        print(f"‚úÖ Contexto criado: {context.project_id}")
        
        # Salvar contexto
        success = context_manager.save_project_context(context, create_snapshot=True)
        print(f"‚úÖ Contexto salvo: {success}")
        
        # Carregar contexto
        loaded_context = context_manager.load_project_context(context.project_id)
        print(f"‚úÖ Contexto carregado: {loaded_context.project_name}")
        
        # Estat√≠sticas
        stats = context_manager.get_manager_stats()
        print(f"‚úÖ Estat√≠sticas: {stats['total_contexts']} contextos, hit rate: {stats['cache_hit_rate']:.2%}")
        
        # Cleanup
        context_manager.cleanup_and_shutdown()
        
        return True
    except Exception as e:
        print(f"‚ùå Erro no teste de context manager: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Fun√ß√£o principal de teste"""
    print("üöÄ Iniciando testes do sistema Evolux Engine")
    print("=" * 60)
    
    tests = [
        test_basic_imports,
        test_configuration,
        test_observability,
        test_security,
        test_llm_routing,
        test_project_scaffolding,
        test_context_manager
    ]
    
    passed = 0
    failed = 0
    
    for test in tests:
        try:
            if test():
                passed += 1
            else:
                failed += 1
        except Exception as e:
            print(f"‚ùå Erro inesperado em {test.__name__}: {e}")
            failed += 1
    
    print("\n" + "=" * 60)
    print(f"üìä Resultados dos testes:")
    print(f"‚úÖ Passou: {passed}")
    print(f"‚ùå Falhou: {failed}")
    print(f"üìà Taxa de sucesso: {passed/(passed+failed)*100:.1f}%")
    
    if failed == 0:
        print("\nüéâ Todos os testes passaram! O sistema est√° funcionando corretamente.")
        return 0
    else:
        print(f"\n‚ö†Ô∏è  {failed} teste(s) falharam. Verifique os erros acima.")
        return 1

if __name__ == "__main__":
    sys.exit(main())