Entendido! Você quer os arquivos completos aplicando as sugestões para torná-los assíncronos e usando o `LLMClient` aprimorado, especificamente para a estrutura que você tem atualmente no seu GitHub (`run.py` e `evolux_engine/agent.py`).

**Importante:** Esta adaptação mantém a estrutura do *seu* `agent.py` atual (com um único agente monolítico), mas o torna assíncrono. Isso é diferente da arquitetura V2 mais modular com `Orchestrator`, `PlannerAgent`, etc., que estávamos projetando antes.

Vamos lá:

**1. Arquivo: `evolux_engine/llms/llm_client.py` (Completo e Funcional)**
Este é o mesmo arquivo que forneci na resposta anterior, mas coloco aqui novamente para garantir.

```python
import json
from typing import Optional, Dict, Any, List, Union

import httpx # Para chamadas HTTP assíncronas
from loguru import logger

# Supondo que você tenha contracts.py para LLMProvider
# Se não, você pode definir o Enum aqui ou usar strings diretamente
try:
    from evolux_engine.schemas.contracts import LLMProvider
except ImportError:
    from enum import Enum
    class LLMProvider(Enum):
        OPENAI = "openai"
        OPENROUTER = "openrouter"
        # Adicione outros conforme necessário


class LLMClient:
    """
    Cliente assíncrono para interagir com APIs de LLM,
    atualmente focado em OpenRouter e compatível com a API OpenAI.
    """

    def __init__(
        self,
        api_key: str,
        model_name: str,
        provider: Union[str, LLMProvider] = LLMProvider.OPENROUTER,
        api_base_url: Optional[str] = None,
        timeout: int = 90,
        http_referer: Optional[str] = None,
        x_title: Optional[str] = None,
    ):
        if not api_key:
            logger.error("LLMClient: API key não fornecida na inicialização.")
            raise ValueError("API key é obrigatória para LLMClient.")
        if not model_name:
            logger.error("LLMClient: Nome do modelo não fornecido na inicialização.")
            raise ValueError("Nome do modelo é obrigatório para LLMClient.")

        self.api_key = api_key
        self.model_name = model_name
        self.timeout = timeout

        if isinstance(provider, str):
            try:
                self.provider = LLMProvider[provider.upper()]
            except KeyError:
                logger.warning(f"Provedor LLM desconhecido: '{provider}'. Usando OpenRouter por padrão.")
                self.provider = LLMProvider.OPENROUTER
        else:
            self.provider = provider
        
        self.base_url = api_base_url or self._get_default_base_url()
        
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        if self.provider == LLMProvider.OPENROUTER:
            if http_referer:
                self.headers["HTTP-Referer"] = http_referer
            if x_title:
                self.headers["X-Title"] = x_title
        
        self._async_client: Optional[httpx.AsyncClient] = None
        logger.info(f"LLMClient inicializado para provedor '{self.provider.value}' com modelo padrão '{self.model_name}'.")

    def _get_default_base_url(self) -> str:
        if self.provider == LLMProvider.OPENAI:
            return "https://api.openai.com/v1"
        elif self.provider == LLMProvider.OPENROUTER:
            return "https://openrouter.ai/api/v1"
        else:
            logger.warning(f"URL base não especificada para provedor '{self.provider.value}'. Usando URL do OpenRouter.")
            return "https://openrouter.ai/api/v1"

    async def _get_async_client(self) -> httpx.AsyncClient:
        if self._async_client is None or self._async_client.is_closed:
            self._async_client = httpx.AsyncClient(headers=self.headers, timeout=self.timeout)
        return self._async_client

    async def close(self):
        if self._async_client and not self._async_client.is_closed:
            await self._async_client.aclose()
            self._async_client = None
            logger.info(f"LLMClient HTTPX client para '{self.model_name}' fechado.")

    async def generate_response(
        self,
        messages: List[Dict[str, str]],
        model_override: Optional[str] = None,
        max_tokens: int = 3800, # Aumentado o padrão
        temperature: float = 0.7,
    ) -> Optional[str]:
        model_to_use = model_override or self.model_name
        endpoint_url = f"{self.base_url}/chat/completions"

        payload = {
            "model": model_to_use,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
        }
        # Truncate log message if too long
        last_message_content = messages[-1]['content']
        log_message_preview = (last_message_content[:150] + '...') if len(last_message_content) > 150 else last_message_content

        logger.debug(f"Enviando requisição para LLM: Modelo='{model_to_use}', URL='{endpoint_url}', Última Msg='{log_message_preview}'")

        try:
            client = await self._get_async_client()
            response = await client.post(endpoint_url, json=payload)
            response.raise_for_status()
            
            result_json = response.json()
            
            if result_json.get('choices') and \
               isinstance(result_json['choices'], list) and \
               len(result_json['choices']) > 0 and \
               result_json['choices'][0].get('message') and \
               result_json['choices'][0]['message'].get('content'):
                
                content = result_json['choices'][0]['message']['content']
                usage = result_json.get('usage', {})
                logger.debug(
                    f"Resposta recebida do LLM ('{model_to_use}'). "
                    f"Prompt Tokens: {usage.get('prompt_tokens', 'N/A')}, "
                    f"Completion Tokens: {usage.get('completion_tokens', 'N/A')}, "
                    f"Conteúdo (início): {(content[:150] + '...') if len(content) > 150 else content}"
                )
                return str(content)
            else:
                logger.error(
                    f"Resposta do LLM ('{model_to_use}') em formato inesperado. "
                    f"Endpoint: {endpoint_url}, Resposta: {json.dumps(result_json, indent=2)}"
                )
                return None
                
        except httpx.TimeoutException:
            logger.error(f"Timeout ({self.timeout}s) ao chamar API LLM. Modelo: {model_to_use}, URL: {endpoint_url}")
            return None
        except httpx.HTTPStatusError as http_err:
            error_body_str = "N/A"
            try:
                error_body_json = http_err.response.json()
                error_body_str = json.dumps(error_body_json, indent=2)
            except json.JSONDecodeError:
                error_body_str = http_err.response.text
            logger.error(
                f"Erro HTTP {http_err.response.status_code} ao chamar API LLM. "
                f"Modelo: {model_to_use}, URL: {endpoint_url}, "
                f"Corpo da Resposta:\n{error_body_str}"
            )
            return None
        except httpx.RequestError as req_err:
            logger.error(
                f"Erro na requisição ao chamar API LLM. "
                f"Modelo: {model_to_use}, URL: {endpoint_url}, Erro: {req_err}"
            )
            return None
        except Exception as e:
            logger.opt(exception=True).error(
                f"Erro inesperado ao gerar resposta do LLM. "
                f"Modelo: {model_to_use}, URL: {endpoint_url}"
            )
            return None

    async def list_models(self) -> Optional[List[Dict[str, Any]]]:
        if self.provider not in [LLMProvider.OPENROUTER, LLMProvider.OPENAI]:
            logger.warning(f"Listagem de modelos não é implementada de forma padronizada para '{self.provider.value}'.")
        
        endpoint_url = f"{self.base_url}/models"
        logger.info(f"Listando modelos disponíveis de: {endpoint_url}")

        try:
            client = await self._get_async_client()
            response = await client.get(endpoint_url)
            response.raise_for_status()
            models_data = response.json()
            
            if isinstance(models_data.get('data'), list):
                logger.info(f"Modelos listados com sucesso. Total: {len(models_data['data'])}")
                return models_data['data']
            else:
                logger.warning(f"Formato de resposta inesperado ao listar modelos: {json.dumps(models_data, indent=2)}")
                return models_data # Retorna a estrutura como está
        except Exception: # Captura genérica para simplificar, mas detalha no log
            logger.opt(exception=True).error(f"Falha ao listar modelos de {endpoint_url}")
            return None
```

**2. Arquivo: `evolux_engine/settings.py` (Adaptação das suas configurações)**
Seu `agent.py` usa `settings`. Baseado nas variáveis de ambiente que você mostrou, criei este `settings.py`.

```python
# evolux_engine/settings.py
import os
from dotenv import load_dotenv
from pydantic_settings import BaseSettings, SettingsConfigDict
from pydantic import Field

# Carrega variáveis do arquivo .env para o ambiente do OS
# Isso é feito globalmente uma vez pelo python-dotenv
# O BaseSettings abaixo lerá diretamente do ambiente
# load_dotenv() # Removido, pois run.py já faz isso com log.

class EvoluxSettings(BaseSettings):
    # Configuração para Pydantic BaseSettings
    model_config = SettingsConfigDict(
        env_file='.env',            # Nome do arquivo .env
        env_file_encoding='utf-8',
        extra='ignore',             # Ignorar variáveis de ambiente extras
        case_sensitive=False,       # Nomes de variáveis de ambiente não diferenciam maiúsculas/minúsculas
        env_prefix='EVOLUX_'        # Ler variáveis que começam com EVOLUX_
    )

    # Chaves de API (usando Field para definir o nome da variável de ambiente)
    # O alias precisa ser o nome da variável no .env SEM o prefixo. Pydantic adicionará o prefixo.
    openrouter_api_key: Optional[str] = Field(None, validation_alias='OPENROUTER_API_KEY')
    openai_api_key: Optional[str] = Field(None, validation_alias='OPENAI_API_KEY')
    # Adicione outras chaves de API conforme necessário

    # Diretório base dos projetos
    project_base_dir: str = Field("./project_workspaces", validation_alias='PROJECT_BASE_DIR')

    # Provedor LLM padrão
    llm_provider: str = Field("openrouter", validation_alias='LLM_PROVIDER')

    # Modelos padrão
    model_planner: str = Field("deepseek/deepseek-coder", validation_alias='MODEL_PLANNER') # Exemplo
    model_executor: str = Field("deepseek/deepseek-coder", validation_alias='MODEL_EXECUTOR') # Exemplo
    # model_validator: str = Field("anthropic/claude-3-haiku-20240307", validation_alias='MODEL_VALIDATOR')

    # Configurações do Engine
    max_concurrent_tasks: int = Field(1, validation_alias='MAX_CONCURRENT_TASKS')
    logging_level: str = Field("INFO", validation_alias='LOGGING_LEVEL')

    # Opcional: para OpenRouter
    http_referer: Optional[str] = Field(None, validation_alias='HTTP_REFERER') # Ex: http://localhost
    app_title: Optional[str] = Field("Evolux Engine (TCC)", validation_alias='APP_TITLE')


# Instância global das configurações
settings = EvoluxSettings()

# Log para confirmar carregamento (opcional, pode ser feito no run.py)
# print("Variáveis de ambiente carregadas (via Pydantic settings) em settings.py:")
# for field_name, field_value in settings.model_fields.items():
#     alias = field_value.validation_alias
#     env_var_name_no_prefix = alias if isinstance(alias, str) else (alias.choices[0] if alias and alias.choices else field_name.upper())
#     env_var_name = f"{settings.model_config.get('env_prefix', '').upper()}{env_var_name_no_prefix}"
#     actual_value = getattr(settings, field_name)
#     print(f"  {env_var_name} (campo {field_name}): {'********' if 'KEY' in env_var_name else actual_value}")

```
**Certifique-se de que seu arquivo `.env` tem as variáveis como `EVOLUX_OPENROUTER_API_KEY`, etc.**

**3. Arquivo: `evolux_engine/utils/logging_utils.py` (Adaptado)**
Seu `agent.py` usa `from evolux_engine.utils.logging_utils import log`. Vamos criar uma versão simples.

```python
# evolux_engine/utils/logging_utils.py
import sys
from loguru import logger as loguru_logger # Renomear para evitar conflito

# Remover handlers padrão para configurar do zero
loguru_logger.remove()

# Adicionar um handler para console
loguru_logger.add(
    sys.stderr,
    level="INFO", # Nível padrão, pode ser sobrescrito por configurações
    format="{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: <8} | {name}:{function}:{line} - {message}",
    colorize=True,
)

# Adicionar um handler para arquivo (opcional)
# loguru_logger.add(
# "evolux_agent.log",
# rotation="10 MB",
# retention="7 days",
# level="DEBUG",
# format="{time} | {level} | {module}:{function}:{line} - {message}",
# )

# Exportar o logger configurado
log = loguru_logger

def setup_logging_level(level: str):
    """Permite reconfigurar o nível do logger principal (stderr)."""
    log.remove() # Remove handler existente para reconfigurar com novo nível
    log.add(
        sys.stderr,
        level=level.upper(),
        format="{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: <8} | {name}:{function}:{line} - {message}",
        colorize=True,
    )
    log.info(f"Nível de logging reconfigurado para: {level.upper()}")

```

**4. Arquivo `evolux_engine/context_manager.py` (Mantendo sua estrutura)**
Seu `agent.py` usa `ContextManager`. Vou manter a estrutura do seu GitHub.

```python
# evolux_engine/context_manager.py
import os
import json
from pathlib import Path
from evolux_engine.utils.logging_utils import log

class ContextManager:
    def __init__(self, project_id: str, base_path: str = "project_workspaces"):
        self.project_id = project_id
        self.base_path = Path(base_path)
        self.project_path = self.base_path / self.project_id
        self._ensure_project_dir_exists()
        log.info("ContextManager inicializado para projeto.", path=str(self.project_path), project_id=self.project_id)

    def _ensure_project_dir_exists(self):
        try:
            self.project_path.mkdir(parents=True, exist_ok=True)
            log.info("Diretório do projeto criado/verificado.", path=str(self.project_path), project_id=self.project_id)
        except Exception as e:
            log.error("Erro ao criar diretório do projeto.", path=str(self.project_path), error=str(e))
            raise

    def get_path(self, relative_path: str) -> Path:
        return self.project_path / relative_path

    def ensure_dir_exists(self, relative_dir_path: str):
        """Garante que um subdiretório dentro do projeto exista."""
        dir_path = self.get_path(relative_dir_path)
        try:
            dir_path.mkdir(parents=True, exist_ok=True)
            log.debug(f"Diretório {dir_path} criado/verificado.")
        except Exception as e:
            log.error(f"Erro ao criar diretório {dir_path}.", error=str(e))
            raise

    def save_file(self, relative_filepath: str, content: str) -> Path:
        filepath = self.get_path(relative_filepath)
        try:
            # Garante que o diretório pai exista
            filepath.parent.mkdir(parents=True, exist_ok=True)
            with open(filepath, "w", encoding="utf-8") as f:
                f.write(content)
            log.info("Arquivo salvo.", path=str(filepath), project_id=self.project_id)
            return filepath
        except Exception as e:
            log.error("Erro ao salvar arquivo.", path=str(filepath), error=str(e))
            raise

    def load_file(self, relative_filepath: str) -> str:
        filepath = self.get_path(relative_filepath)
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                content = f.read()
            log.info("Arquivo carregado.", path=str(filepath), project_id=self.project_id)
            return content
        except FileNotFoundError:
            log.warning("Arquivo não encontrado para leitura.", path=str(filepath), project_id=self.project_id)
            return "" # Ou levantar erro
        except Exception as e:
            log.error("Erro ao carregar arquivo.", path=str(filepath), error=str(e))
            raise

    def save_json(self, relative_filepath: str, data: dict):
        filepath = self.get_path(relative_filepath)
        try:
            filepath.parent.mkdir(parents=True, exist_ok=True)
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            log.info("JSON salvo.", path=str(filepath), project_id=self.project_id)
        except Exception as e:
            log.error("Erro ao salvar JSON.", path=str(filepath), error=str(e))
            raise

    def load_json(self, relative_filepath: str) -> dict:
        filepath = self.get_path(relative_filepath)
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                data = json.load(f)
            log.info("JSON carregado.", path=str(filepath), project_id=self.project_id)
            return data
        except FileNotFoundError:
            log.warning("Arquivo JSON não encontrado.", path=str(filepath), project_id=self.project_id)
            return {} # Ou levantar erro
        except json.JSONDecodeError:
            log.error("Erro ao decodificar JSON.", path=str(filepath), project_id=self.project_id)
            return {} # Ou levantar erro
        except Exception as e:
            log.error("Erro ao carregar JSON.", path=str(filepath), error=str(e))
            raise
```

**5. Arquivo: `evolux_engine/agent.py` (Completo e Adaptado para Async)**
Este é o seu `agent.py` do GitHub, modificado para ser assíncrono e usar o `LLMClient` atualizado.

```python
# evolux_engine/agent.py
import re
import uuid
import asyncio # Adicionado
from typing import Optional, List

from evolux_engine.llms.llm_client import LLMClient
from evolux_engine.context_manager import ContextManager
from evolux_engine.utils.logging_utils import log, setup_logging_level
from evolux_engine.settings import settings # Importar configurações

class Agent:
    def __init__(self, project_id: str, goal: str):
        self.project_id = project_id
        self.goal = goal
        self.context_manager = ContextManager(project_id, base_path=settings.project_base_dir)
        self.project_structure_str: str = "" # Estrutura de arquivos/diretórios como string

        # Configurar nível de logging baseado nas settings
        setup_logging_level(settings.logging_level)

        log.info(f"Inicializando LLMClients para Agente {project_id}")
        # Usar valores das 'settings'
        self.planner_llm = LLMClient(
            api_key=settings.openrouter_api_key, 
            model_name=settings.model_planner,
            provider=settings.llm_provider,
            http_referer=settings.http_referer,
            x_title=settings.app_title
        )
        log.info(f"LLM client (planner) criado para {settings.llm_provider} com modelo {settings.model_planner}.", agent_id=self.project_id, goal=self.goal)
        
        self.executor_llm = LLMClient(
            api_key=settings.openrouter_api_key, 
            model_name=settings.model_executor,
            provider=settings.llm_provider,
            http_referer=settings.http_referer,
            x_title=settings.app_title
        )
        log.info(f"LLM client (executor) criado para {settings.llm_provider} com modelo {settings.model_executor}.", agent_id=self.project_id, goal=self.goal)
        log.info(f"Agente inicializado. Provedor LLM: {settings.llm_provider}. Status: Executor LLM: OK, Planner LLM: OK", agent_id=self.project_id, goal=self.goal)


    async def _generate_project_structure(self):
        log.info("Iniciando geração da estrutura do projeto.", agent_id=self.project_id, goal=self.goal)
        prompt = (
            f"Baseado no seguinte objetivo do projeto: '{self.goal}', "
            f"gere uma estrutura de arquivos e diretórios adequada. "
            f"Se for um projeto de código, sugira os arquivos principais. Se for um documento, talvez apenas um arquivo. "
            f"Liste APENAS os nomes dos arquivos e diretórios, um por linha. Marque diretórios com '/' no final. "
            f"Não inclua explicações adicionais, apenas a lista de caminhos.\n"
            f"Exemplo para um app Python simples:\n"
            f"app/\n"
            f"app/__init__.py\n"
            f"app/main.py\n"
            f"app/utils.py\n"
            f"tests/\n"
            f"tests/test_main.py\n"
            f"requirements.txt\n"
            f"README.md"
        )
        log.info("Enviando prompt para estrutura do projeto...", agent_id=self.project_id, goal=self.goal)
        
        messages = [{"role": "user", "content": prompt}]
        response = await self.planner_llm.generate_response(messages=messages, max_tokens=500)
        
        if response:
            self.project_structure_str = response.strip()
            log.info(f"Estrutura do projeto recebida:\n{self.project_structure_str}", agent_id=self.project_id, goal=self.goal)
        else:
            log.error("Nenhuma resposta recebida do LLM (planner) para a estrutura do projeto.", agent_id=self.project_id, goal=self.goal)
            self.project_structure_str = "" 

    def _extract_code_from_response(self, response_text: str, language: Optional[str] = None) -> Optional[str]:
        if not response_text:
            return None

        # Tentativa de extrair de blocos de código específicos
        if language:
            pattern = re.compile(rf"```{language}\s*([\s\S]*?)\s*```", re.MULTILINE)
            match = pattern.search(response_text)
            if match:
                return match.group(1).strip()

        # Tentativa de extrair de blocos de código genéricos
        pattern_generic = re.compile(r"```(?:\w+)?\s*([\s\S]*?)\s*```", re.MULTILINE)
        match_generic = pattern_generic.search(response_text)
        if match_generic:
            return match_generic.group(1).strip()
        
        # Se nenhum bloco de código for encontrado, e o prompt pediu para não ter explicações,
        # pode ser que a LLM tenha retornado o conteúdo diretamente.
        # Mas só se o prompt foi muito claro sobre "APENAS o conteúdo".
        # Para ser mais seguro, vamos retornar None se nenhum bloco foi encontrado,
        # a menos que uma lógica mais sofisticada seja adicionada.
        # Pode-se assumir que se não há ``` e não há muitas frases explicativas, é o conteúdo.
        # Por agora, mantemos a extração por ```
        
        # Se a resposta não contiver "```" e for relativamente curta e sem frases tipo "aqui está o código",
        # poderia ser o conteúdo direto. Isso é mais arriscado.
        # if "```" not in response_text and len(response_text.splitlines()) > 1 and not any(
        #    phrase in response_text.lower() for phrase in ["of course", "here is", "i have generated"]
        # ):
        #    return response_text.strip()

        return None # Não encontrou bloco de código claro

    async def _generate_file_content(self, filepath_str: str, file_description: Optional[str] = None):
        log.info(f"Iniciando geração do conteúdo para {filepath_str}.", agent_id=self.project_id, goal=self.goal)
        
        file_extension = filepath_str.split('.')[-1] if '.' in filepath_str else "txt"
        language_map = {
            "py": "python", "js": "javascript", "html": "html", "css": "css",
            "md": "markdown", "json": "json", "yaml": "yaml", "yml": "yaml",
            "txt": "text", "sh": "bash", "java": "java", "c": "c", "cpp": "cpp",
            "go": "go", "rs": "rust", "ts": "typescript"
        }
        language_hint = language_map.get(file_extension, file_extension) # Para o bloco de código ```<language>


        if not file_description:
            file_description = f"Conteúdo completo e funcional para o arquivo '{filepath_str}' como parte do projeto com objetivo: '{self.goal}'."
            if self.project_structure_str: # Adicionar contexto da estrutura se disponível
                file_description += f"\nO projeto possui a seguinte estrutura de arquivos (o arquivo atual é '{filepath_str}'):\n{self.project_structure_str}"

        prompt = (
            f"Você é um assistente especialista em desenvolvimento de software e criação de conteúdo. "
            f"Gere o conteúdo completo e funcional para o arquivo chamado '{filepath_str}'.\n"
            f"Descrição/Requisitos para este arquivo: {file_description}\n"
            f"Objetivo geral do projeto: {self.goal}\n"
            f"IMPORTANTE: Forneça APENAS o conteúdo do arquivo, sem explicações adicionais. "
            f"O conteúdo deve estar dentro de um único bloco de código formatado como ```{language_hint} ... ```. "
            f"Por exemplo, para um arquivo Python, use ```python ... ```. Para HTML, ```html ... ```. Para texto simples, ```text ... ```. "
            f"Não adicione nenhuma frase introdutória ou conclusiva fora do bloco de código."
        )
        log.info(f"Enviando prompt para conteúdo de {filepath_str}...", agent_id=self.project_id, goal=self.goal)

        messages = [{"role": "user", "content": prompt}]
        # Aumente max_tokens se espera arquivos muito grandes, mas cuidado com custos/limites
        content_response = await self.executor_llm.generate_response(messages=messages, max_tokens_override=settings.MAX_TOKENS_EXECUTOR) 
        
        final_content = None
        if content_response:
            final_content = self._extract_code_from_response(content_response, language_hint)
            if not final_content and language_hint != "text": # Tentar com 'text' se hint específico falhou
                 final_content = self._extract_code_from_response(content_response, "text")
            if not final_content: # Última tentativa: genérico
                 final_content = self._extract_code_from_response(content_response)


        if not final_content:
            raw_response_path_str = f"{filepath_str}.raw_response.txt"
            self.context_manager.save_file(raw_response_path_str, content_response or "Nenhuma resposta recebida da LLM.")
            preview = (content_response[:500] + "...") if content_response and len(content_response) > 500 else (content_response or 'VAZIO')
            log.warning(f"Nenhum bloco de código '{language_hint}' (ou alternativo) encontrado na resposta para '{filepath_str}'. "
                        f"Resposta bruta salva em: {raw_response_path_str}. Início da resposta bruta:\n{preview}",
                        agent_id=self.project_id, goal=self.goal)
            # DECISÃO: Usar a resposta bruta como fallback?
            # Se o prompt for muito bem obedecido, a LLM pode retornar SÓ o conteúdo sem ```
            # Por ora, vamos considerar falha se não extrair do bloco, mas isso pode ser ajustado.
            # final_content = content_response # DESCOMENTE PARA USAR RESPOSTA BRUTA COMO FALLBACK

        if final_content:
            full_path = self.context_manager.save_file(filepath_str, final_content)
            log.info(f"Conteúdo para {filepath_str} gerado e salvo em {full_path}.", agent_id=self.project_id, goal=self.goal)
            return True
        else:
            log.error(f"Falha ao gerar ou extrair conteúdo para {filepath_str}.", agent_id=self.project_id, goal=self.goal)
            return False

    async def run(self):
        log.info("Iniciando execução do agente.", agent_id=self.project_id, goal=self.goal)
        try:
            await self._generate_project_structure()

            if not self.project_structure_str:
                log.error("Não foi possível gerar a estrutura do projeto. Abortando a geração de arquivos.", agent_id=self.project_id)
                return # Não prosseguir se a estrutura falhou

            files_to_generate: List[str] = []
            # Parse self.project_structure_str. Simples split por linha.
            # Isso assume que a LLM retornou uma lista de caminhos, um por linha.
            parsed_files = [line.strip() for line in self.project_structure_str.split('\n') if line.strip()]
            
            for item_path_str in parsed_files:
                if item_path_str.endswith('/'):
                    # É um diretório, garantir que exista
                    log.info(f"Garantindo que o diretório '{item_path_str}' exista.")
                    self.context_manager.ensure_dir_exists(item_path_str)
                else:
                    # É um arquivo para gerar
                    files_to_generate.append(item_path_str)
            
            if not files_to_generate and not any(p.endswith('/') for p in parsed_files):
                log.warning("Nenhum arquivo específico identificado na estrutura do projeto retornada pela LLM. "
                            "Verifique o formato da resposta da LLM para a estrutura.", agent_id=self.project_id)
                # Se o objetivo era um único arquivo HTML, e a LLM retornou só "index.html"
                # Ou se a LLM retornou uma descrição em vez de uma lista de arquivos.
                # Para o seu objetivo específico, talvez um fallback seja criar 'index.html'
                if "página html" in self.goal.lower() and not any(f.lower().endswith(".html") for f in files_to_generate):
                    log.info("Objetivo parece ser uma página HTML, adicionando 'index.html' à lista de geração como fallback.")
                    files_to_generate.append("index.html")


            if not files_to_generate:
                log.error("Nenhum arquivo para gerar após processar a estrutura do projeto. Verifique a resposta da LLM de planejamento.", agent_id=self.project_id)
                return

            log.info(f"Arquivos a serem gerados: {files_to_generate}", agent_id=self.project_id)

            for filepath_str in files_to_generate:
                # A descrição do arquivo pode vir da LLM de planejamento ou ser genérica
                description_for_file = f"Conteúdo para o arquivo '{filepath_str}' no projeto '{self.goal}'."
                if filepath_str.lower() == "readme.md":
                    description_for_file = (f"Um arquivo README.md detalhado para o projeto com o objetivo: '{self.goal}'. "
                                           f"Deve incluir uma visão geral do projeto, como usá-lo ou executá-lo (se aplicável), "
                                           f"e um resumo da estrutura de arquivos planejada:\n{self.project_structure_str}")
                elif filepath_str.lower().endswith(".html"):
                     description_for_file = (f"O conteúdo HTML completo para o arquivo '{filepath_str}'. "
                                             f"Este arquivo faz parte de um projeto com o objetivo: '{self.goal}'. "
                                             f"O HTML deve ser bem estruturado, semanticamente correto e visualmente agradável. "
                                             f"Se o objetivo mencionou 'pelo menos 1000 linhas de texto e com imagens', "
                                             f"certifique-se de incluir conteúdo substancial e placeholders para imagens (ex: <img src='placeholder.jpg' alt='Descrição'>) "
                                             f"ou sugestões de onde buscar imagens ou como elas deveriam ser. "
                                             f"Inclua uma introdução, corpo principal com várias seções e, se apropriado, uma conclusão. "
                                             f"Se for sobre 'evolução da espécie humana', cubra os principais marcos.")


                await self._generate_file_content(filepath_str, description_for_file)

            log.info("Execução do agente concluída.", agent_id=self.project_id, goal=self.goal)
        
        except Exception as e:
            log.opt(exception=True).error(f"Erro durante a execução do agente {self.project_id}.", error=str(e))
        
        finally:
            log.info(f"Fechando clientes LLM para Agente {self.project_id}...")
            if self.planner_llm:
                await self.planner_llm.close()
            if self.executor_llm:
                await self.executor_llm.close()
            log.info(f"Clientes LLM para Agente {self.project_id} fechados.")
```

**6. Arquivo: `run.py` (Completo e Adaptado para Async)**
Este é o seu `run.py` do GitHub, modificado para ser assíncrono.

```python
# run.py
import argparse
import asyncio # Adicionado
import uuid
from dotenv import load_dotenv

# Adiciona o diretório src ao sys.path para importações diretas se run.py estiver na raiz
# import sys
# from pathlib import Path
# sys.path.append(str(Path(__file__).parent / "evolux_engine")) # Descomente se necessário

# Importar settings primeiro para que sejam carregadas antes dos outros módulos
from evolux_engine.settings import settings
from evolux_engine.utils.logging_utils import log, setup_logging_level # Log e setup
from evolux_engine.agent import Agent


async def main(): # Modificado para async
    # Carregar .env antes de tudo (embora settings.py também o faça via Pydantic)
    # Se settings.py já usa Pydantic com env_file, esta chamada aqui pode ser redundante
    # mas não prejudica.
    if load_dotenv():
        log.info("Arquivo .env carregado por python-dotenv.")
    else:
        log.warning("Arquivo .env não encontrado ou não carregado por python-dotenv.")

    # Configurar nível de logging do console baseado nas settings globais
    setup_logging_level(settings.logging_level)
    log.info(f"Logging configurado para o nível: {settings.logging_level.upper()}")

    log.info("Evolux Engine iniciando...")

    parser = argparse.ArgumentParser(description="Evolux Engine - Agente de Desenvolvimento Autônomo")
    parser.add_argument("--goal", type=str, required=True, help="O objetivo principal do projeto.")
    parser.add_argument("--project_id", type=str, default=None, help="ID de um projeto existente para continuar ou novo ID.")
    args = parser.parse_args()

    project_id_to_use = args.project_id if args.project_id else str(uuid.uuid4())
    
    log.info(f"Objetivo recebido: {args.goal}", project_id_arg=args.project_id)

    # A inicialização do ContextManager já cria o diretório base do projeto
    # Se precisar logar a criação explicitamente, pode ser feito no ContextManager

    log.info("Inicializando Agente...", agent_id=project_id_to_use, goal=args.goal)
    
    agent = None # Para o bloco finally
    try:
        agent = Agent(project_id=project_id_to_use, goal=args.goal)
        log.info("Agente instanciado. Iniciando execução...")
        
        await agent.run() # Modificado para await

        log.info("Execução do agente principal concluída.", project_id=project_id_to_use) # Sucesso aqui é relativo ao término sem crash
    
    except Exception as e:
        log.opt(exception=True).critical(f"Erro crítico na execução principal do run.py: {e}")
    
    # O fechamento dos clientes LLM agora está dentro do agent.run() no bloco finally.
    # Se a instanciação do Agent falhar, o finally do Agent não será chamado.
    # Para robustez, se a instanciação do agent pudesse falhar *antes* dos LLMClients serem criados
    # ou se os LLMClients fossem gerenciados aqui, um try/finally aqui para os clientes seria bom.
    # Mas com a estrutura atual, o finally no agent.run() é o local mais próximo.

if __name__ == "__main__":
    asyncio.run(main()) # Modificado para asyncio.run()
```

**Para Testar:**

1.  **Verifique seu `.env`:**
    Deve ter no mínimo:
    ```env
    EVOLUX_OPENROUTER_API_KEY="sk-or-v1-sua-chave-aqui"
    EVOLUX_MODEL_PLANNER="deepseek/deepseek-r1-0528-qwen3-8b:free" # ou outro modelo que você usa
    EVOLUX_MODEL_EXECUTOR="deepseek/deepseek-r1-0528-qwen3-8b:free"
    EVOLUX_PROJECT_BASE_DIR="./project_workspaces"
    EVOLUX_LOGGING_LEVEL="DEBUG" # Use DEBUG para ver mais detalhes!
    EVOLUX_LLM_PROVIDER="openrouter"
    # Opcional para OpenRouter
    # EVOLUX_HTTP_REFERER="http://localhost" 
    # EVOLUX_APP_TITLE="Evolux TCC Test"
    EVOLUX_MAX_TOKENS_EXECUTOR=3800 # Adicionei esta no agent.py para controlar tokens do executor
    ```
    Adicione `EVOLUX_MAX_TOKENS_EXECUTOR=3800` ao seu `.env` se não estiver lá.

2.  **Estrutura de Pastas:**
    Certifique-se que sua estrutura de pastas está assim:
    ```
    seu_projeto_raiz/
    ├── .env
    ├── run.py
    └── evolux_engine/
        ├── __init__.py  (pode estar vazio)
        ├── agent.py
        ├── context_manager.py
        ├── settings.py
        ├── llms/
        │   ├── __init__.py (pode estar vazio)
        │   └── llm_client.py
        └── utils/
            ├── __init__.py (pode estar vazio)
            └── logging_utils.py
    ```

3.  **Instale `httpx` e `pydantic-settings`:**
    ```bash
    pip install httpx pydantic-settings loguru python-dotenv
    ```

4.  **Execute:**
    ```bash
    python run.py --goal "Criar uma página html simples sobre a evolução da espécie humana, com introdução e algumas seções. Deve ter um título principal e parágrafos."
    ```
    Modifiquei o goal para ser um pouco menos ambicioso que "1000 linhas" para o teste inicial.

Com `EVOLUX_LOGGING_LEVEL="DEBUG"`, você deverá ver as mensagens trocadas com a LLM ou, mais importante, o corpo do erro HTTP se ele ocorrer. Preste atenção aos logs!